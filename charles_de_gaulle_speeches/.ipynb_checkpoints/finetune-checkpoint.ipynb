{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5865a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32438621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b765a65",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2228c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaning/dataset.csv')\n",
    "df = df.rename(columns={'0': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee8384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discours sur la politique de rigueur, 28 décembre 1958     Le 21 décembre, le général de Gaulle a été élu Président de la République française et de  la  Communauté.  En  annonçant  qu'il  accepte  le  mandat  qui  lui  a  été  confié,  il  expose  comment il l'accomplira et donne les raisons de la politique financière de rigueur qui va  être mise en œuvre.    Avant tout, Françaises, Français, je veux vous dire que j'accepte le mandat que vous m'avez  confié. Votre décision fut marquée lors de la crise nationale du mois de mai, affirmée par le  référendum,  répétée  par  les  élections,  précisée  par  le  vote  des  élus  dimanche  dernier.  La  tâche nationale qui m'incombe depuis dix-huit ans se trouve, de ce fait, confirmée. Guide de  la France et Chef de l'État républicain, j'exercerai le pouvoir suprême dans toute l'étendue  qu'il comporte désormais et suivant l'esprit nouveau qui me l'a fait attribuer.  L'appel  qui  m'est  adressé  par  le  pays  exprime  son  instinct  du  salut.  S'il  me  charge  de  le  conduire, c'est parce qu'il veut aller, non certes à la facilité, mais à l'effort et au renouveau.  Depuis sept mois, assez remplis, nous y allons, en effet, et faisons quelques grands pas sur  la  voie  du  redressement.  En  vérité,  il  était  temps.  Car,  à  la  veille  du  démarrage,  l'unité  française se trouvait sur le point de se briser, entraînant tout à l'effondrement.  Bien entendu, le danger s'étendait aux finances et à l'économie. Dans les derniers jours de  mai, nous étions, à cet égard, sur la route de la catastrophe. La balance des comptes entre ce  qu'il  nous  fallait  acheter  au-dehors  et  ce  que  nous  pouvions  y  vendre  atteignait  un  déficit  apparemment  irréductible,  tandis  que  les  prêts  étrangers  se  trouvaient  presque  épuisés.  D'ailleurs, on ne voyait plus comment faire face normalement à toutes les dépenses de l'État,  les impôts n'y suffisant point et le crédit semblant s'éteindre. Enfin, les prix ne cessaient pas  de  monter,  ainsi  que  l'inquiétude  sociale.  Brochant  sur  le  tout,  une  certaine  récession  se  faisait déjà sentir. Le mouvement du mois de mai, s'il apparut d'abord en Algérie, procédait  en  réalité  de  la  conviction  générale  que  le  pouvoir  était  impuissant  devant  la  marée  des  menaces, y compris, naturellement, celles qui pesaient sur notre économie.  La confiance du pays nous a permis, dans ce domaine comme dans les autres, de renverser  la tendance et de parer au plus pressé. Cependant, la situation demeure précaire, c'est-à-dire  dangereuse. Quand on mesure les espoirs que nous apportent la nombreuse jeunesse venue  au  monde  depuis  la  guerre,  le  pétrole,  le  gaz  et  l'uranium  découverts,  notre  outillage  en  progrès,  nos  élites  nouvelles  surgissant  du  fond  du  peuple,  notre  association  avec  la  Communauté,  l'élargissement  imminent  du  marché  européen,  on  est  saisi  à  la  fois  par  l'impatience et la résolution.  Avec  mon  gouvernement,  j'ai  donc  pris  la  décision  de  mettre  nos  affaires  en  ordre  réellement et profondément. Le budget en est l'occasion, peut-être ultime, très bonne en tout  cas.  Nous  avons  adopté  et,  demain,  nous  appliquerons  tout  un  ensemble  de  mesures  financières, économiques, sociales, qui établit la nation sur une base de vérité et de sévérité,  la seule qui puisse lui permettre de bâtir sa prospérité. Je ne cache pas que notre pays va se  trouver quelque temps à l'épreuve. Mais le rétablissement visé est tel qu'il peut nous payer  de tout.  Au  point  de  vue  des  charges  publiques,  rien  ne  saurait  être  accepté  qui  aboutisse  à  l'inflation. Mais, en même temps, tout doit être fait pour poursuivre, et même pour accroître,  les investissements qui commandent notre avenir, soit dans le domaine social : logements,  écoles, hôpitaux, soit dans le domaine économique : énergie, équipement, communications.  En  outre,  nous  avons  entrepris  de  transformer  l'Algérie  tandis  qu'avance  la  pacification.  Encore. nous faut-il concourir à la mise en valeur des pays de la Communauté. Enfin, nous  ne pouvons pas, dans l'état où est l'univers, nous dispenser d'une force militaire importante.  Mais toutes ces obligations, jointes aux dépenses normales de l'État, comporteraient, si nous   \f",
      "laissions aller les choses, un déficit de 1 200 milliards, soit deux fois plus que l'épargne n'est  susceptible  de  nous  fournir.  A  moins  de  recourir  à  la  ruineuse  inflation  ou  de  faillir  à  la  France, il n'y a rien d'autre à faire que de réduire de moitié le déficit menaçant. C'est ce qui  est décidé.  Tout à l'heure, M. Pinay, ministre des Finances et des Affaires économiques, vous indiquera  avec précision quelles dispositions sont prises. En voici l'essentiel. Accroître les impôts sur  les  sociétés  et  sur  les  revenus  élevés.  Taxer  le  vin,  l'alcool,  le  tabac.  Supprimer  maintes  subventions  accordées  par  le  Trésor  et  qui  s'appliquent,  en  particulier,  à  des  produits  de  consommation.  Réduire  la  participation  du  budget  au  financement  des  entreprises  nationalisées,  notamment  des  chemins  de  fer.  Combler,  dans  le  fonctionnement  des  assurances  sociales,  le  déficit  dont  les  fonds  publics  ont  à  supporter  la  charge.  Inviter  les  anciens combattants qui sont pourvus du nécessaire et qui ne sont pas invalides à renoncer à  leur retraite ; les pensions des veuves, des orphelins, des mutilés, restant, bien entendu, ce  qu'elles sont. Abolir une série de ces indexations qui ne sont, en réalité, que proclamations  de méfiance à l'égard de la monnaie. En revanche, augmenter de 245 milliards, c'est-à-dire  de 25 %, le montant des investissements qui sont notre richesse future et ouvrent carrière à  notre  jeunesse.  Voilà  de  rudes  dispositions  !  Je  ne  sais  que  trop  ce  que,  dans  l'immédiat,  elles  coûteront  à  beaucoup.  Mais  je  les  crois  efficaces,  je  les  tiens  pour  nécessaires,  et  je  suis sûr qu'en définitive tout le monde tirera profit de l'équilibre ainsi réalisé.  Comme on peut s'attendre à ce qu'il en résulte, au moins momentanément, quelque hausse  du niveau des prix, le gouvernement prend aussi des mesures relatives au pouvoir d'achat,  de  celui  surtout  des  Français  dont  le  revenu  est  le  plus  modeste.  Le  1er  février,  le  salaire  minimum  interprofessionnel  garanti  sera  accru  en  conséquence,  tandis  que,  dès  le  1er  janvier, les vieilles gens verront leur retraite majorée de 5 200 francs. J'ajoute que va être  créé, par coopération du patronat et des syndicats, un fonds spécial destiné au maintien de  l'emploi et assurant aux travailleurs qui se trouveraient en chômage, un complément portant  l'allocation aux environs du salaire minimum. Quant aux personnels dépendant directement  de l'État : fonction publique, services, entreprises nationalisées, il est prévu, qu'à partir du  1er février, leur rémunération sera, \"ne varietur\", augmentée de 4 %.  Dans  le  monde  d'aujourd'hui,  rien  ne  vaut  que  par  comparaison.  Or,  nous  sommes  actuellement,  vis-à-vis  de  l'extérieur,  dans  une  situation  économique  diminuée.  Empêchés  d'importer  et  d'exporter  suffisamment,  endettés,  privés  de  crédit,  alors  que  nos  produits,  notre technique, nos capacités, sont à hauteur de toutes les concurrences, nous sommes loin  d'atteindre  au  large  niveau  d'échanges  qui  développerait  notre  activité.  Ce  qu'il  y  a  d'artificiel dans la valeur de notre monnaie provient, certes, du déséquilibre de nos affaires,  mais n'en est pas moins une cause permanente de difficultés.  C'est pourquoi, tout en remédiant au désordre fondamental, nous devons placer notre franc  sur  une  base  telle  qu'il  soit  inébranlable.  Nous  le  faisons  donc,  regrettant  d'en  abaisser  le  taux,  mais  tirant  les  conséquences  de  négligences  prolongées.  Du  même  coup,  notre  monnaie devient convertible au-dehors en toutes monnaies étrangères, en même temps et au  même titre que plusieurs de nos partenaires européens le font pour leur livre, leur mark, leur  lire, leur florin, etc. Au surplus, le vieux franc français, si souvent mutilé à mesure de nos  vicissitudes, nous voulons qu'il reprenne une substance conforme au respect qui lui est dû.  Avant la fin de 1959, apparaîtra le franc nouveau valant cent francs d'aujourd'hui. Quant aux  échanges avec l'étranger nous avons pris la décision de les libérer à 90 % ouvrant les vannes  au  courant  et  replaçant  décidément  la  France,  dans  le  domaine  économique,  à  son  rang  international.  Cet  ensemble,  dont  toutes  les  parties  se  tiennent  et  se  complètent,  cet  ensemble  est  grave  mais  essentiel.  Sans  l'effort  de  remise  en  ordre,  avec  les  sacrifices  qu'il  requiert  et  les  espoirs qu'il comporte, nous resterons un pays à la traîne, oscillant perpétuellement entre le   \f",
      "drame et la médiocrité. Au contraire, si nous réussissons la grande entreprise nationale de  redressement  financier  et  économique,  quelle  étape  sur  la  route  qui  nous  mène  vers  les  sommets ! C'est le voeu que je forme à l'adresse de toutes et de tous tandis qu'approche la  nouvelle année.  Peuple français, grand peuple ! fierté, courage, espérance  Vive la République !  Vive la France !\n"
     ]
    }
   ],
   "source": [
    "df = df.replace(r'\\n',' ', regex=True) \n",
    "print(df.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554455df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d53e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 89\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aece292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef37841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 71\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3491fa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('antoinelouis/belgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321ff48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \n",
    "    return tokenizer([x for x in examples['text']], truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b55b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a05d04fe4e14f59b2afd2dd197580d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f2ab4aa4b34d0f87955dcc3e895ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e119d279df4942b78e9f36ef59c40928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a50e479be624f72855c4c7bf1197bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e30603be1c64458860099e0ced3bdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69ab8c83c8467b98063cb277628227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d010a65985cc40feaecd234ff366a16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479cfe4784c94361beed1341db73f348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedbcf06a3224893aa4475d729155d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10a95438213477b8d8a81ee4e0eb1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bd6ccdf84b4ceab664b63b8c1233e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf802f571214f1c979f3e471e04a9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64adc3f5f5e4f40a6ced9c97943507d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4816ce89a20e450d88933b871c6f78f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda6f039ae3643f89a9ea1cc9408179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4a894da60e46b7afe642bd2e2c074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036bf3e4aa50472cabe77208a69fef52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f645338d2604e0b88c898d1ccce74e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bca93598e44079b28287ea3286a17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8da87a972432295a075d4b666e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c9c14dd0ef43eca1781fa44b4f4c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723b563415ef415c8d4da05c1300f18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa8c2cc192445309dbb3d9b58ffbd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf1d14719047589912c502009e6d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "\n",
    "    preprocess_function,\n",
    "\n",
    "    batched=True,\n",
    "\n",
    "    num_proc=12,\n",
    "\n",
    "    remove_columns = dataset[\"train\"].column_names,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee7223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 71\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e5e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "\n",
    "        for k, t in concatenated_examples.items()\n",
    "\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec8b858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58460ff69c36437fb9c8f0a3ee1323cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd581663d7c4c06820d9793257d55ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af79091cde84806984e36b7c23a2ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aab7e38fad240e1b548e3df7a1cd01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162877dce56f4a51a21e50d68926987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac29fca878584c8e80eff8cada34e6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a83e04e7004af4833a258a7e76191d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1409e2b01ce40499851076bcbb61d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998e2121306144d68d2d8392cbe0a64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2bfc9a67564f849fa5586107deade4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b227c402085a4d29ac5f11d7353efb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e9fb239d2749cda93be75ab84a705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ec9bbde21546b09f48b865552585eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c2f165b1ac438ea8b609ddef7a9978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3592cadf074135baf990f4ba47c84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19247bf842854d04b6a537c8360f443c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782c298f2f994406845b7ef4b375d062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ddfaaf03364558843f19be4dd935bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7d6041d3ed4657af0ac3bad562081d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e3e64eeefc4eb08e1ae59aa6c72fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70573a08b51e452a90d1d36a00a9ef4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00192f30675466c9775f2a212e51ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e816338b8194895b658ef6c59092d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a9a864adf34fd88826dae67eefc4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b619f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 852\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 239\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "574ce86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32734bea",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774ac700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('antoinelouis/belgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89b0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"CharlesDeGaulle-GPT\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=11,\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=64\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4db53e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=CharlesDeGaulle-GPT/runs/Jan16_17-01-18_pop-os,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=50,\n",
      "optim=adamw_hf,\n",
      "output_dir=CharlesDeGaulle-GPT,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=CharlesDeGaulle-GPT,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomaslemenestrel/Documents/charles_de_gaulle_speeches/charles_de_gaulle_speeches/CharlesDeGaulle-GPT is already a clone of https://huggingface.co/tlemenestrel/CharlesDeGaulle-GPT. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/thomaslemenestrel/miniconda3/envs/cdg/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 852\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='541' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [541/700 05:04 < 01:29, 1.77 it/s, Epoch 38.57/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.338810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.988494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.836293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.750417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.702802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.671404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.647637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.638451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.633578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.625381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.623163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.626813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.624624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.631367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.631879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.636988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.644889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.647159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.655047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.670116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.665322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.673766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.682019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.692465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.696587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.697150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.705677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.720480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.719362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.727682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.731417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.740293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.743305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.740875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.750072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.034500</td>\n",
       "      <td>2.754241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.034500</td>\n",
       "      <td>2.764779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.034500</td>\n",
       "      <td>2.765179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to CharlesDeGaulle-GPT/checkpoint-500\n",
      "Configuration saved in CharlesDeGaulle-GPT/checkpoint-500/config.json\n",
      "Model weights saved in CharlesDeGaulle-GPT/checkpoint-500/pytorch_model.bin\n",
      "Several commits (3) will be pushed upstream.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 239\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "\n",
    "    data_collator=data_collator\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92484ef3",
   "metadata": {},
   "source": [
    "### Save to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.push_to_hub()\n",
    "#tokenizer.push_to_hub('https://huggingface.co/tlemenestrel/CharlesDeGaulle-GPT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc666c",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Les gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"CharlesDeGaulle-GPT\")\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
